services:
  backend:
    build: ./backend
    container_name: backend
    ports:
      - "8000:8000"
    volumes:
      - ./backend:/app
      # Persist huggingface/transformers cache so model files are not re-downloaded every build
      - ./hf_cache:/opt/hf_cache
    # Enable GPU access in Compose v2+; Docker must support GPUs on host.
    gpus: all
    # Increase shared memory and ulimits to help large model loading
    shm_size: '2g'
    ulimits:
      memlock: -1
      stack: 67108864
    environment:
      # Model selection - 8B is a good fit for RTX 3080 with 8-bit quantization
      - QWEN_VL_MODEL=Qwen/Qwen3-VL-2B-Instruct
      # Load model in bitsandbytes 8-bit mode (recommended for 10GB VRAM)
      - LOAD_IN_8BIT=false
      # Generation hyperparameters (tweak if OOM)
      - GEN_TOP_P=0.6
      - GEN_TOP_K=10
      - GEN_TEMPERATURE=0.25
      - GEN_REPETITION_PENALTY=1.0
      - GEN_PRESENCE_PENALTY=1.2
      # Keep this conservative to start â€” increase if you have memory headroom
      - GEN_MAX_NEW_TOKENS=80
      # Optional: enable remote inference fallback (set to "true" and provide HF_TOKEN)
      - USE_REMOTE_INFERENCE=false
      - HF_TOKEN=
      # Make sure transformers/huggingface use the mounted cache directory
      - HF_HOME=/opt/hf_cache
      - TRANSFORMERS_CACHE=/opt/hf_cache/transformers
      - HF_DATASETS_CACHE=/opt/hf_cache/datasets
      - QWEN_VL_LOCAL_PATH=/opt/models/qwen3-vl-2b-instruct
    networks:
      - app-network

  frontend:
    build: ./photoai
    container_name: frontend
    ports:
      - "3000:3000"
    environment:
      - NODE_NO_WARNINGS=1
      - CHOKIDAR_USEPOLLING=true
      - WATCHPACK_POLLING=true
      - NEXT_TELEMETRY_DISABLED=1
      # Point frontend to backend container when in compose mode
      - NEXT_PUBLIC_BACKEND_URL=http://backend:8000
    volumes:
    - ./photoai:/app
    - /app/node_modules

    depends_on:
      - backend
    networks:
      - app-network

networks:
  app-network:
    driver: bridge
